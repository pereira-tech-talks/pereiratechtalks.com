---
publishDate: 2024-05-30T23:30:00.000Z
author: Meetup.com
title: 'Revolucionando el Deep Learning: Potenciando modelos con datos limitados'
image: '~/assets/images/posts/banners/highres_521131998.jpeg'
venue: 'Catholic University of Pereira'
category: Eventos
tags:
  - Meetup.com
metadata:
  canonical: https://pereiratechtalks.com/301101493-revolucionando-el-de
---


<p>¡No te pierdas nuestra próxima reunión de networking y aprendizaje! Únete a nosotros el 30 de mayo a las 6:30 p.m. para descubrir los paradigmas emergentes en la revolución del deep learning y la inteligencia artificial.</p> <p>**¿Qué necesitas traer?** Solo tus ganas de devorar algunos snacks, conocer gente interesante y aprender sobre las últimas tendencias en tecnología. ¡Nosotros nos encargamos del resto!<br/>**Ubicación y cómo encontrarnos:** Nos reuniremos en Universidad Católica de Pereira en la Sala de estudiantes. Un espacio ideal para la interacción y el aprendizaje.</p> <p>**Charlas:**</p> <p>**Charla 1:**</p> <p>**Speaker:** Leiver Campeon<br/>**Rol:** Expert Intelligence cómo Machine learning engineer y Tech Lead</p> <p>**Continual Learning and Catastrophic Forgetting in Modern AI Systems**<br/>Después de desplegar un modelo de Deep Learning, nos encontramos con el desafío de aprender continuamente nuevas tareas sin perder el conocimiento previamente adquirido. En esta charla, exploraremos estrategias para superar el olvido catastrófico y garantizar que nuestros modelos evolucionen de manera efectiva.</p> <p>**Charla 2:**</p> <p>**Speaker:** Sebastián Franco Gómez<br/>**Rol:** ML Engineer en Expert Intelligence</p> <p>**Optimizando modelos de Deep Learning: Cuando la data no es tan abundante**<br/>Resumen: El Deep Learning ha supuesto una revolución en el campo del Machine Learning y la inteligencia artificial en la última década, parte de la razón de por qué ha sido tan exitoso es por la explosión de data disponible, entre más data podamos ingerir a un modelo mejor se comportará, sin embargo, en el mundo real nos enfrentaremos a problemas con data específica y escasa. ¿Cómo podemos usar los superpoderes del Deep Learning intentando burlar su necesidad de ingentes cantidades de datos?</p> 
